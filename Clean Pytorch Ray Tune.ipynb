{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ebf8167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "# import numpy as np\n",
    "import requests\n",
    "import datetime\n",
    "import ray\n",
    "from ray import tune, air\n",
    "\n",
    "import sys\n",
    "sys.path.append('./data')\n",
    "sys.path.append('./training')\n",
    "import data, training\n",
    "\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "006e27a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split as TTS\n",
    "# import ray\n",
    "# from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "# import torch.distributed as dist\n",
    "# import torch.multiprocessing as mp\n",
    "# import wandb\n",
    "# from tqdm.auto import tqdm, trange\n",
    "# from ray.air import session\n",
    "\n",
    "\n",
    "# from ray.train.torch import TorchCheckpoint\n",
    "# from ray.train import Trainer\n",
    "\n",
    "# # from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "# from ray.tune.search.bohb import TuneBOHB\n",
    "# from ray.tune.schedulers import ASHAScheduler\n",
    "# from ray.air.callbacks.wandb import WandbLoggerCallback\n",
    "# from functools import partial\n",
    "\n",
    "# import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86b35134",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=96\n",
    "WINDOW_SIZE=32\n",
    "NUM_WORKERS=6\n",
    "N_FEATURES=1\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e855c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 10:00:45,506\tINFO worker.py:1509 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.10.6</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.0.1</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8265\" target=\"_blank\">http://127.0.0.1:8265</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.10.6', ray_version='2.0.1', ray_commit='03b6bc7b5a305877501110ec04710a9c57011479', address_info={'node_ip_address': '10.0.0.153', 'raylet_ip_address': '10.0.0.153', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2023-04-12_10-00-43_300526_52481/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2023-04-12_10-00-43_300526_52481/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2023-04-12_10-00-43_300526_52481', 'metrics_export_port': 58283, 'gcs_address': '10.0.0.153:52978', 'address': '10.0.0.153:52978', 'dashboard_agent_listen_port': 52365, 'node_id': 'f33757e262a6bd98904597bb33d6d83b00d895cd6b033b65662bcd83'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(num_cpus=int(os.cpu_count() * .75), num_gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e3392a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/ray/dashboard/agent.py:470: DeprecationWarning: There is no current event loop\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   loop = asyncio.get_event_loop()\n"
     ]
    }
   ],
   "source": [
    "# Need to go requests route to avoid forbidden error\n",
    "url = 'https://stockanalysis.com/list/biggest-companies/'\n",
    "\n",
    "header = {\n",
    "  \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\",\n",
    "  \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "}\n",
    "\n",
    "r = requests.get(url, headers=header)\n",
    "\n",
    "dfs = pd.read_html(r.text)\n",
    "\n",
    "symbols = list(dfs[0]['Symbol'].iloc[:25].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebbc5072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  25 of 25 completed\n",
      "\n",
      "1 Failed download:\n",
      "- BRK.B: No timezone found, symbol may be delisted\n"
     ]
    }
   ],
   "source": [
    "# Maximum number of days for YFinance 5m data is 60, but it only takes 59 day diff\n",
    "end_date = datetime.date.today() \n",
    "start_date = end_date - datetime.timedelta(days=59)\n",
    "\n",
    "dataset = yf.download(\n",
    "    symbols,\n",
    "    period='max', \n",
    "    interval='5m',\n",
    "    start=start_date, \n",
    "    end=end_date\n",
    ")['Adj Close']\n",
    "\n",
    "train, val = data.get_train_val_dicts(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc9967f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'device': DEVICE,\n",
    "    'D_MODEL': tune.choice([32, 64]),\n",
    "    'N_HEADS': tune.choice([2, 8]),\n",
    "    'D_FF': tune.choice([128, 256]),\n",
    "    'DROPOUT': tune.choice([0., .05, .1, .15]),\n",
    "    'ACTIVATION': tune.choice(['relu', 'gelu']),\n",
    "    'NUM_LAYERS': tune.choice([3, 5]),\n",
    "    'LR': tune.choice([1e-3, 1e-4, 1e-5]),\n",
    "    'WINDOW_SIZE': tune.choice([32, 64])\n",
    "}\n",
    "param_space = {'train_loop_config': config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3df8460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 10:00:53,098\tWARNING bohb_search.py:152 -- You passed a `space` parameter to <class 'ray.tune.search.bohb.bohb_search.TuneBOHB'> that contained unresolved search space definitions. <class 'ray.tune.search.bohb.bohb_search.TuneBOHB'> should however be instantiated with fully configured search spaces only. To use Ray Tune's automatic search space conversion, pass the space definition as part of the `param_space` argument to `tune.Tuner()` instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2023-04-12 10:03:09 (running for 00:02:16.24)<br>Memory usage on this node: 10.0/62.4 GiB<br>Using HyperBand: num_stopped=0 total_brackets=2\n",
       "Round #0:\n",
       "  Bracket(Max Size (n)=80, Milestone (r)=3, completed=1.6%): {PAUSED: 6, PENDING: 1, RUNNING: 1} <br>Resources requested: 1.0/9 CPUs, 1.0/1 GPUs, 0.0/33.73 GiB heap, 0.0/16.86 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /media/alexander/38F66E50F66E0E86/Users/Alexander/Downloads/Python Projects/medium_articles/tfmr_logger/TorchTrainer_2023-04-12_10-00-53<br>Number of trials: 8/8 (6 PAUSED, 1 PENDING, 1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc             </th><th>train_loop_config...  </th><th style=\"text-align: right;\">  train_loop_config...</th><th style=\"text-align: right;\">  train_loop_config...</th><th style=\"text-align: right;\">  train_loop_config...</th><th style=\"text-align: right;\">  train_loop_config/LR</th><th style=\"text-align: right;\">  train_loop_config...</th><th style=\"text-align: right;\">  train_loop_config...</th><th style=\"text-align: right;\">  train_loop_config...</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  val_loss</th><th style=\"text-align: right;\">  _timestamp</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_ad153ce4</td><td>RUNNING </td><td>10.0.0.153:53910</td><td>gelu                  </td><td style=\"text-align: right;\">                  0.1 </td><td style=\"text-align: right;\">                   128</td><td style=\"text-align: right;\">                    64</td><td style=\"text-align: right;\">                1e-05 </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                     8</td><td style=\"text-align: right;\">                    64</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">            </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">            </td></tr>\n",
       "<tr><td>TorchTrainer_7027d7ec</td><td>PAUSED  </td><td>10.0.0.153:52947</td><td>relu                  </td><td style=\"text-align: right;\">                  0.05</td><td style=\"text-align: right;\">                   128</td><td style=\"text-align: right;\">                    64</td><td style=\"text-align: right;\">                1e-05 </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">                    64</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         15.2203</td><td style=\"text-align: right;\">    136.887 </td><td style=\"text-align: right;\">  133.332 </td><td style=\"text-align: right;\">  1681308071</td></tr>\n",
       "<tr><td>TorchTrainer_7236c520</td><td>PAUSED  </td><td>10.0.0.153:53104</td><td>gelu                  </td><td style=\"text-align: right;\">                  0   </td><td style=\"text-align: right;\">                   256</td><td style=\"text-align: right;\">                    32</td><td style=\"text-align: right;\">                1e-05 </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                     8</td><td style=\"text-align: right;\">                    32</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         17.8565</td><td style=\"text-align: right;\">     56.0579</td><td style=\"text-align: right;\">   54.2382</td><td style=\"text-align: right;\">  1681308093</td></tr>\n",
       "<tr><td>TorchTrainer_7b9a2e36</td><td>PAUSED  </td><td>10.0.0.153:53267</td><td>gelu                  </td><td style=\"text-align: right;\">                  0.15</td><td style=\"text-align: right;\">                   128</td><td style=\"text-align: right;\">                    64</td><td style=\"text-align: right;\">                1e-05 </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                     8</td><td style=\"text-align: right;\">                    64</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         18.6535</td><td style=\"text-align: right;\">    136.899 </td><td style=\"text-align: right;\">  133.398 </td><td style=\"text-align: right;\">  1681308116</td></tr>\n",
       "<tr><td>TorchTrainer_88b787a8</td><td>PAUSED  </td><td>10.0.0.153:53428</td><td>gelu                  </td><td style=\"text-align: right;\">                  0.15</td><td style=\"text-align: right;\">                   128</td><td style=\"text-align: right;\">                    32</td><td style=\"text-align: right;\">                0.0001</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                     8</td><td style=\"text-align: right;\">                    32</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         16.7603</td><td style=\"text-align: right;\">     56.4745</td><td style=\"text-align: right;\">   54.5568</td><td style=\"text-align: right;\">  1681308136</td></tr>\n",
       "<tr><td>TorchTrainer_95d3f296</td><td>PAUSED  </td><td>10.0.0.153:53595</td><td>gelu                  </td><td style=\"text-align: right;\">                  0.15</td><td style=\"text-align: right;\">                   256</td><td style=\"text-align: right;\">                    32</td><td style=\"text-align: right;\">                0.001 </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">                    64</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         14.4136</td><td style=\"text-align: right;\">    137.273 </td><td style=\"text-align: right;\">  133.83  </td><td style=\"text-align: right;\">  1681308154</td></tr>\n",
       "<tr><td>TorchTrainer_a25971b2</td><td>PAUSED  </td><td>10.0.0.153:53750</td><td>relu                  </td><td style=\"text-align: right;\">                  0.15</td><td style=\"text-align: right;\">                   128</td><td style=\"text-align: right;\">                    64</td><td style=\"text-align: right;\">                1e-05 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">                    64</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         15.062 </td><td style=\"text-align: right;\">    137.319 </td><td style=\"text-align: right;\">  134.001 </td><td style=\"text-align: right;\">  1681308173</td></tr>\n",
       "<tr><td>TorchTrainer_b867bf86</td><td>PENDING </td><td>                </td><td>gelu                  </td><td style=\"text-align: right;\">                  0.1 </td><td style=\"text-align: right;\">                   128</td><td style=\"text-align: right;\">                    64</td><td style=\"text-align: right;\">                0.001 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">                     8</td><td style=\"text-align: right;\">                    32</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">            </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">            </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m 2023-04-12 10:00:58,958\tINFO config.py:71 -- Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m wandb: wandb version 0.14.2 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m wandb: Tracking run with wandb version 0.13.4\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m wandb: Run data is saved locally in /media/alexander/38F66E50F66E0E86/Users/Alexander/Downloads/Python Projects/medium_articles/tfmr_logger/TorchTrainer_2023-04-12_10-00-53/TorchTrainer_7027d7ec_1_ACTIVATION=relu,DROPOUT=0.0500,D_FF=128,D_MODEL=64,LR=0.0000,NUM_LAYERS=5,N_HEADS=2,WINDOW_SIZE=64_2023-04-12_10-00-53/rank_0/wandb/run-20230412_100101-2qql1mm9\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m wandb: Syncing run stoic-brook-63\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/lavellea/torch_stock_transformer\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m wandb: 🚀 View run at https://wandb.ai/lavellea/torch_stock_transformer/runs/2qql1mm9\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m 2023-04-12 10:01:07,896\tINFO train_loop_utils.py:300 -- Moving model to device: cuda:0\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]m \n",
      "  9%|▉         | 1/11 [00:00<00:05,  1.80it/s]\n",
      " 36%|███▋      | 4/11 [00:00<00:00,  7.28it/s]\n",
      " 64%|██████▎   | 7/11 [00:00<00:00, 11.80it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 11.89it/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m /home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/torch/nn/modules/transformer.py:569: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m   return torch._transformer_encoder_layer_fwd(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for TorchTrainer_7027d7ec:\n",
      "  _time_this_iter_s: 9.664819955825806\n",
      "  _timestamp: 1681308069\n",
      "  _training_iteration: 1\n",
      "  date: 2023-04-12_10-01-09\n",
      "  done: false\n",
      "  experiment_id: 366d518997094ed6b511f5dff8dab268\n",
      "  hostname: Ajax\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.0.0.153\n",
      "  pid: 52947\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 13.066022157669067\n",
      "  time_this_iter_s: 13.066022157669067\n",
      "  time_total_s: 13.066022157669067\n",
      "  timestamp: 1681308069\n",
      "  timesteps_since_restore: 0\n",
      "  train_loss: 138.00783746892756\n",
      "  training_iteration: 1\n",
      "  trial_id: 7027d7ec\n",
      "  val_loss: 133.90579223632812\n",
      "  warmup_time: 0.0022439956665039062\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]m \n",
      " 27%|██▋       | 3/11 [00:00<00:00, 27.75it/s]\n",
      " 55%|█████▍    | 6/11 [00:00<00:00, 28.40it/s]\n",
      " 82%|████████▏ | 9/11 [00:00<00:00, 28.51it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 28.81it/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m /home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/torch/nn/modules/transformer.py:569: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m   return torch._transformer_encoder_layer_fwd(\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]m \n",
      " 27%|██▋       | 3/11 [00:00<00:00, 25.75it/s]\n",
      " 55%|█████▍    | 6/11 [00:00<00:00, 23.41it/s]\n",
      " 82%|████████▏ | 9/11 [00:00<00:00, 23.69it/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m Exception in thread MsgRouterThr:\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/threading.py\", line 953, in run\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/wandb/sdk/interface/router.py\", line 70, in message_loop\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m     msg = self._read_message()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/wandb/sdk/interface/router_queue.py\", line 36, in _read_message\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m     msg = self._response_queue.get(timeout=1)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/multiprocessing/queues.py\", line 117, in get\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m     res = self._recv_bytes()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/multiprocessing/connection.py\", line 217, in recv_bytes\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m     self._check_closed()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/multiprocessing/connection.py\", line 141, in _check_closed\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52986)\u001b[0m     raise OSError(\"handle is closed\")\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m 2023-04-12 10:01:17,868\tINFO config.py:71 -- Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m wandb: Currently logged in as: lavellea. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m wandb: wandb version 0.14.2 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m wandb: Tracking run with wandb version 0.13.4\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m wandb: Run data is saved locally in /media/alexander/38F66E50F66E0E86/Users/Alexander/Downloads/Python Projects/medium_articles/tfmr_logger/TorchTrainer_2023-04-12_10-00-53/TorchTrainer_7236c520_2_ACTIVATION=gelu,DROPOUT=0.0000,D_FF=256,D_MODEL=32,LR=0.0000,NUM_LAYERS=5,N_HEADS=8,WINDOW_SIZE=32_2023-04-12_10-01-12/rank_0/wandb/run-20230412_100120-2nklnszd\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m wandb: Syncing run robust-plant-64\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/lavellea/torch_stock_transformer\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m wandb: 🚀 View run at https://wandb.ai/lavellea/torch_stock_transformer/runs/2nklnszd\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m 2023-04-12 10:01:27,652\tINFO train_loop_utils.py:300 -- Moving model to device: cuda:0\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]m \n",
      "  5%|▍         | 1/21 [00:00<00:10,  1.94it/s]\n",
      " 24%|██▍       | 5/21 [00:00<00:01,  9.58it/s]\n",
      " 43%|████▎     | 9/21 [00:00<00:00, 15.70it/s]\n",
      " 62%|██████▏   | 13/21 [00:00<00:00, 20.69it/s]\n",
      " 81%|████████  | 17/21 [00:00<00:00, 24.75it/s]\n",
      "100%|██████████| 21/21 [00:01<00:00, 19.20it/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m /home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/torch/nn/modules/transformer.py:569: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m   return torch._transformer_encoder_layer_fwd(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for TorchTrainer_7236c520:\n",
      "  _time_this_iter_s: 11.079203605651855\n",
      "  _timestamp: 1681308089\n",
      "  _training_iteration: 1\n",
      "  date: 2023-04-12_10-01-29\n",
      "  done: false\n",
      "  experiment_id: 278342379a6844b39d6455161aa84e91\n",
      "  hostname: Ajax\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.0.0.153\n",
      "  pid: 53104\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 14.405511856079102\n",
      "  time_this_iter_s: 14.405511856079102\n",
      "  time_total_s: 14.405511856079102\n",
      "  timestamp: 1681308089\n",
      "  timesteps_since_restore: 0\n",
      "  train_loss: 56.64122935703823\n",
      "  training_iteration: 1\n",
      "  trial_id: 7236c520\n",
      "  val_loss: 54.56273814610073\n",
      "  warmup_time: 0.0022296905517578125\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]m \n",
      " 19%|█▉        | 4/21 [00:00<00:00, 36.91it/s]\n",
      " 38%|███▊      | 8/21 [00:00<00:00, 37.03it/s]\n",
      " 57%|█████▋    | 12/21 [00:00<00:00, 36.83it/s]\n",
      " 76%|███████▌  | 16/21 [00:00<00:00, 36.94it/s]\n",
      "100%|██████████| 21/21 [00:00<00:00, 37.46it/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m /home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/torch/nn/modules/transformer.py:569: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m   return torch._transformer_encoder_layer_fwd(\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]m \n",
      " 19%|█▉        | 4/21 [00:00<00:00, 36.60it/s]\n",
      " 38%|███▊      | 8/21 [00:00<00:00, 36.67it/s]\n",
      " 57%|█████▋    | 12/21 [00:00<00:00, 36.79it/s]\n",
      " 76%|███████▌  | 16/21 [00:00<00:00, 35.63it/s]\n",
      "100%|██████████| 21/21 [00:00<00:00, 36.81it/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m /home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/torch/nn/modules/transformer.py:569: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m   return torch._transformer_encoder_layer_fwd(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m Exception in thread MsgRouterThr:\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/threading.py\", line 953, in run\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/wandb/sdk/interface/router.py\", line 70, in message_loop\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m     msg = self._read_message()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/wandb/sdk/interface/router_queue.py\", line 36, in _read_message\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m     msg = self._response_queue.get(timeout=1)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/multiprocessing/queues.py\", line 117, in get\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m     res = self._recv_bytes()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/multiprocessing/connection.py\", line 217, in recv_bytes\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m     self._check_closed()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/multiprocessing/connection.py\", line 141, in _check_closed\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m     raise OSError(\"handle is closed\")\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53144)\u001b[0m OSError: handle is closed\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m 2023-04-12 10:01:39,848\tINFO config.py:71 -- Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m wandb: wandb version 0.14.2 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m wandb: Tracking run with wandb version 0.13.4\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m wandb: Run data is saved locally in /media/alexander/38F66E50F66E0E86/Users/Alexander/Downloads/Python Projects/medium_articles/tfmr_logger/TorchTrainer_2023-04-12_10-00-53/TorchTrainer_7b9a2e36_3_ACTIVATION=gelu,DROPOUT=0.1500,D_FF=128,D_MODEL=64,LR=0.0000,NUM_LAYERS=5,N_HEADS=8,WINDOW_SIZE=64_2023-04-12_10-01-34/rank_0/wandb/run-20230412_100144-34r3p9ld\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m wandb: Syncing run dark-eon-65\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/lavellea/torch_stock_transformer\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m wandb: 🚀 View run at https://wandb.ai/lavellea/torch_stock_transformer/runs/34r3p9ld\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m 2023-04-12 10:01:52,186\tINFO train_loop_utils.py:300 -- Moving model to device: cuda:0\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]m \n",
      "  9%|▉         | 1/11 [00:00<00:05,  1.86it/s]\n",
      " 27%|██▋       | 3/11 [00:00<00:01,  5.66it/s]\n",
      " 55%|█████▍    | 6/11 [00:00<00:00, 10.42it/s]\n",
      " 82%|████████▏ | 9/11 [00:00<00:00, 14.07it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 11.43it/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m /home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/torch/nn/modules/transformer.py:569: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m   return torch._transformer_encoder_layer_fwd(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for TorchTrainer_7b9a2e36:\n",
      "  _time_this_iter_s: 13.073561191558838\n",
      "  _timestamp: 1681308113\n",
      "  _training_iteration: 1\n",
      "  date: 2023-04-12_10-01-53\n",
      "  done: false\n",
      "  experiment_id: 5331537ac5294d759dfb8a7dcaa1b8b1\n",
      "  hostname: Ajax\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.0.0.153\n",
      "  pid: 53267\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 16.4312481880188\n",
      "  time_this_iter_s: 16.4312481880188\n",
      "  time_total_s: 16.4312481880188\n",
      "  timestamp: 1681308113\n",
      "  timesteps_since_restore: 0\n",
      "  train_loss: 138.36467812278053\n",
      "  training_iteration: 1\n",
      "  trial_id: 7b9a2e36\n",
      "  val_loss: 133.72323989868164\n",
      "  warmup_time: 0.0025186538696289062\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]m \n",
      " 27%|██▋       | 3/11 [00:00<00:00, 25.93it/s]\n",
      " 55%|█████▍    | 6/11 [00:00<00:00, 24.42it/s]\n",
      " 82%|████████▏ | 9/11 [00:00<00:00, 25.31it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 25.48it/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m /home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/torch/nn/modules/transformer.py:569: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m   return torch._transformer_encoder_layer_fwd(\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]m \n",
      " 27%|██▋       | 3/11 [00:00<00:00, 24.75it/s]\n",
      " 55%|█████▍    | 6/11 [00:00<00:00, 21.61it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 25.00it/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m /home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/torch/nn/modules/transformer.py:569: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m   return torch._transformer_encoder_layer_fwd(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m Exception in thread MsgRouterThr:\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/threading.py\", line 953, in run\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/wandb/sdk/interface/router.py\", line 70, in message_loop\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m     msg = self._read_message()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/wandb/sdk/interface/router_queue.py\", line 36, in _read_message\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m     msg = self._response_queue.get(timeout=1)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/multiprocessing/queues.py\", line 117, in get\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m     res = self._recv_bytes()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/multiprocessing/connection.py\", line 217, in recv_bytes\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m     self._check_closed()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/multiprocessing/connection.py\", line 141, in _check_closed\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m     raise OSError(\"handle is closed\")\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53308)\u001b[0m OSError: handle is closed\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m 2023-04-12 10:02:01,813\tINFO config.py:71 -- Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m wandb: Currently logged in as: lavellea. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m wandb: wandb version 0.14.2 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m wandb: Tracking run with wandb version 0.13.4\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m wandb: Run data is saved locally in /media/alexander/38F66E50F66E0E86/Users/Alexander/Downloads/Python Projects/medium_articles/tfmr_logger/TorchTrainer_2023-04-12_10-00-53/TorchTrainer_88b787a8_4_ACTIVATION=gelu,DROPOUT=0.1500,D_FF=128,D_MODEL=32,LR=0.0001,NUM_LAYERS=3,N_HEADS=8,WINDOW_SIZE=32_2023-04-12_10-01-56/rank_0/wandb/run-20230412_100204-286eaxq1\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m wandb: Syncing run ethereal-snowball-66\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/lavellea/torch_stock_transformer\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m wandb: 🚀 View run at https://wandb.ai/lavellea/torch_stock_transformer/runs/286eaxq1\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m 2023-04-12 10:02:10,750\tINFO train_loop_utils.py:300 -- Moving model to device: cuda:0\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]m \n",
      "  5%|▍         | 1/21 [00:00<00:10,  1.86it/s]\n",
      " 29%|██▊       | 6/21 [00:00<00:01, 11.40it/s]\n",
      " 52%|█████▏    | 11/21 [00:00<00:00, 19.06it/s]\n",
      " 76%|███████▌  | 16/21 [00:00<00:00, 24.98it/s]\n",
      "100%|██████████| 21/21 [00:01<00:00, 20.59it/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m /home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/torch/nn/modules/transformer.py:569: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m   return torch._transformer_encoder_layer_fwd(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for TorchTrainer_88b787a8:\n",
      "  _time_this_iter_s: 10.171322345733643\n",
      "  _timestamp: 1681308132\n",
      "  _training_iteration: 1\n",
      "  date: 2023-04-12_10-02-12\n",
      "  done: false\n",
      "  experiment_id: c5bdd5b1951d4f12b8c6063d2920d7be\n",
      "  hostname: Ajax\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.0.0.153\n",
      "  pid: 53428\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 13.435338735580444\n",
      "  time_this_iter_s: 13.435338735580444\n",
      "  time_total_s: 13.435338735580444\n",
      "  timestamp: 1681308132\n",
      "  timesteps_since_restore: 0\n",
      "  train_loss: 57.90740930466425\n",
      "  training_iteration: 1\n",
      "  trial_id: 88b787a8\n",
      "  val_loss: 55.33403778076172\n",
      "  warmup_time: 0.0024292469024658203\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]m \n",
      " 19%|█▉        | 4/21 [00:00<00:00, 39.50it/s]\n",
      " 38%|███▊      | 8/21 [00:00<00:00, 39.42it/s]\n",
      " 57%|█████▋    | 12/21 [00:00<00:00, 39.25it/s]\n",
      " 76%|███████▌  | 16/21 [00:00<00:00, 39.22it/s]\n",
      "100%|██████████| 21/21 [00:00<00:00, 40.01it/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m /home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/torch/nn/modules/transformer.py:569: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m   return torch._transformer_encoder_layer_fwd(\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]m \n",
      " 19%|█▉        | 4/21 [00:00<00:00, 37.31it/s]\n",
      " 38%|███▊      | 8/21 [00:00<00:00, 38.65it/s]\n",
      " 57%|█████▋    | 12/21 [00:00<00:00, 39.00it/s]\n",
      " 76%|███████▌  | 16/21 [00:00<00:00, 39.02it/s]\n",
      "100%|██████████| 21/21 [00:00<00:00, 39.60it/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m /home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/torch/nn/modules/transformer.py:569: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m   return torch._transformer_encoder_layer_fwd(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m Exception in thread MsgRouterThr:\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/threading.py\", line 953, in run\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/wandb/sdk/interface/router.py\", line 70, in message_loop\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m     msg = self._read_message()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/wandb/sdk/interface/router_queue.py\", line 36, in _read_message\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m     msg = self._response_queue.get(timeout=1)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/multiprocessing/queues.py\", line 117, in get\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m     res = self._recv_bytes()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/multiprocessing/connection.py\", line 217, in recv_bytes\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m     self._check_closed()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/multiprocessing/connection.py\", line 141, in _check_closed\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m     raise OSError(\"handle is closed\")\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53467)\u001b[0m OSError: handle is closed\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m 2023-04-12 10:02:22,871\tINFO config.py:71 -- Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m wandb: Currently logged in as: lavellea. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m wandb: wandb version 0.14.2 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m wandb: Tracking run with wandb version 0.13.4\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m wandb: Run data is saved locally in /media/alexander/38F66E50F66E0E86/Users/Alexander/Downloads/Python Projects/medium_articles/tfmr_logger/TorchTrainer_2023-04-12_10-00-53/TorchTrainer_95d3f296_5_ACTIVATION=gelu,DROPOUT=0.1500,D_FF=256,D_MODEL=32,LR=0.0010,NUM_LAYERS=5,N_HEADS=2,WINDOW_SIZE=64_2023-04-12_10-02-17/rank_0/wandb/run-20230412_100224-lrydxao6\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m wandb: Syncing run graceful-yogurt-67\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/lavellea/torch_stock_transformer\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m wandb: 🚀 View run at https://wandb.ai/lavellea/torch_stock_transformer/runs/lrydxao6\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m 2023-04-12 10:02:31,376\tINFO train_loop_utils.py:300 -- Moving model to device: cuda:0\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]m \n",
      " 36%|███▋      | 4/11 [00:00<00:00,  7.68it/s]\n",
      " 64%|██████▎   | 7/11 [00:00<00:00, 12.59it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 12.69it/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m /home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/torch/nn/modules/transformer.py:569: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m   return torch._transformer_encoder_layer_fwd(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for TorchTrainer_95d3f296:\n",
      "  _time_this_iter_s: 9.143475770950317\n",
      "  _timestamp: 1681308152\n",
      "  _training_iteration: 1\n",
      "  date: 2023-04-12_10-02-33\n",
      "  done: false\n",
      "  experiment_id: 89c0a50e22834b81ae75a24bb05f1e1f\n",
      "  hostname: Ajax\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.0.0.153\n",
      "  pid: 53595\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 12.466220617294312\n",
      "  time_this_iter_s: 12.466220617294312\n",
      "  time_total_s: 12.466220617294312\n",
      "  timestamp: 1681308153\n",
      "  timesteps_since_restore: 0\n",
      "  train_loss: 139.24953668767756\n",
      "  training_iteration: 1\n",
      "  trial_id: 95d3f296\n",
      "  val_loss: 134.23497009277344\n",
      "  warmup_time: 0.0022466182708740234\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]m \n",
      " 36%|███▋      | 4/11 [00:00<00:00, 32.96it/s]\n",
      " 73%|███████▎  | 8/11 [00:00<00:00, 32.91it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 34.40it/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m /home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/torch/nn/modules/transformer.py:569: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m   return torch._transformer_encoder_layer_fwd(\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]m \n",
      " 36%|███▋      | 4/11 [00:00<00:00, 32.85it/s]\n",
      " 73%|███████▎  | 8/11 [00:00<00:00, 32.94it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 34.29it/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m /home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/torch/nn/modules/transformer.py:569: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m   return torch._transformer_encoder_layer_fwd(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m Exception in thread MsgRouterThr:\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/threading.py\", line 953, in run\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53635)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/wandb/sdk/interface/router.py\", line 70, in message_loop\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m 2023-04-12 10:02:40,911\tINFO config.py:71 -- Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m wandb: Currently logged in as: lavellea. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m wandb: wandb version 0.14.2 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m wandb: Tracking run with wandb version 0.13.4\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m wandb: Run data is saved locally in /media/alexander/38F66E50F66E0E86/Users/Alexander/Downloads/Python Projects/medium_articles/tfmr_logger/TorchTrainer_2023-04-12_10-00-53/TorchTrainer_a25971b2_6_ACTIVATION=relu,DROPOUT=0.1500,D_FF=128,D_MODEL=64,LR=0.0000,NUM_LAYERS=3,N_HEADS=2,WINDOW_SIZE=64_2023-04-12_10-02-35/rank_0/wandb/run-20230412_100243-355zbk1j\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m wandb: Syncing run divine-dew-68\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/lavellea/torch_stock_transformer\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m wandb: 🚀 View run at https://wandb.ai/lavellea/torch_stock_transformer/runs/355zbk1j\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m 2023-04-12 10:02:50,049\tINFO train_loop_utils.py:300 -- Moving model to device: cuda:0\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]m \n",
      "  9%|▉         | 1/11 [00:00<00:05,  1.88it/s]\n",
      " 45%|████▌     | 5/11 [00:00<00:00,  9.48it/s]\n",
      " 82%|████████▏ | 9/11 [00:00<00:00, 15.57it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 13.40it/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m /home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/torch/nn/modules/transformer.py:569: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m   return torch._transformer_encoder_layer_fwd(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for TorchTrainer_a25971b2:\n",
      "  _time_this_iter_s: 9.635966062545776\n",
      "  _timestamp: 1681308171\n",
      "  _training_iteration: 1\n",
      "  date: 2023-04-12_10-02-51\n",
      "  done: false\n",
      "  experiment_id: 8b9e82109f594add8a2d0601e26241df\n",
      "  hostname: Ajax\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.0.0.153\n",
      "  pid: 53750\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 12.996700763702393\n",
      "  time_this_iter_s: 12.996700763702393\n",
      "  time_total_s: 12.996700763702393\n",
      "  timestamp: 1681308171\n",
      "  timesteps_since_restore: 0\n",
      "  train_loss: 139.89222994717684\n",
      "  training_iteration: 1\n",
      "  trial_id: a25971b2\n",
      "  val_loss: 135.53860092163086\n",
      "  warmup_time: 0.002370595932006836\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]m \n",
      " 36%|███▋      | 4/11 [00:00<00:00, 34.76it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 36.63it/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m /home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/torch/nn/modules/transformer.py:569: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m   return torch._transformer_encoder_layer_fwd(\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]m \n",
      " 36%|███▋      | 4/11 [00:00<00:00, 33.27it/s]\n",
      " 73%|███████▎  | 8/11 [00:00<00:00, 33.05it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 34.99it/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m /home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/torch/nn/modules/transformer.py:569: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m   return torch._transformer_encoder_layer_fwd(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m Exception in thread MsgRouterThr:\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/threading.py\", line 953, in run\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/wandb/sdk/interface/router.py\", line 70, in message_loop\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m     msg = self._read_message()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/wandb/sdk/interface/router_queue.py\", line 36, in _read_message\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m     msg = self._response_queue.get(timeout=1)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/multiprocessing/queues.py\", line 117, in get\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m     res = self._recv_bytes()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/multiprocessing/connection.py\", line 217, in recv_bytes\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m     self._check_closed()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m   File \"/home/alexander/miniconda3/envs/hyperband/lib/python3.10/multiprocessing/connection.py\", line 141, in _check_closed\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m     raise OSError(\"handle is closed\")\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53790)\u001b[0m OSError: handle is closed\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53951)\u001b[0m 2023-04-12 10:03:00,021\tINFO config.py:71 -- Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53951)\u001b[0m wandb: Currently logged in as: lavellea. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53951)\u001b[0m wandb: wandb version 0.14.2 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53951)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53951)\u001b[0m wandb: Tracking run with wandb version 0.13.4\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53951)\u001b[0m wandb: Run data is saved locally in /media/alexander/38F66E50F66E0E86/Users/Alexander/Downloads/Python Projects/medium_articles/tfmr_logger/TorchTrainer_2023-04-12_10-00-53/TorchTrainer_ad153ce4_7_ACTIVATION=gelu,DROPOUT=0.1000,D_FF=128,D_MODEL=64,LR=0.0000,NUM_LAYERS=5,N_HEADS=8,WINDOW_SIZE=64_2023-04-12_10-02-54/rank_0/wandb/run-20230412_100302-1f3j47qq\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53951)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53951)\u001b[0m wandb: Syncing run sweet-shape-69\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53951)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/lavellea/torch_stock_transformer\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53951)\u001b[0m wandb: 🚀 View run at https://wandb.ai/lavellea/torch_stock_transformer/runs/1f3j47qq\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53951)\u001b[0m 2023-04-12 10:03:09,079\tINFO train_loop_utils.py:300 -- Moving model to device: cuda:0\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]m \n",
      "  9%|▉         | 1/11 [00:00<00:05,  1.67it/s]\n",
      " 27%|██▋       | 3/11 [00:00<00:01,  5.19it/s]\n",
      " 55%|█████▍    | 6/11 [00:00<00:00,  9.76it/s]\n",
      " 82%|████████▏ | 9/11 [00:00<00:00, 13.08it/s]\n",
      "100%|██████████| 11/11 [00:01<00:00, 10.27it/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53951)\u001b[0m /home/alexander/miniconda3/envs/hyperband/lib/python3.10/site-packages/torch/nn/modules/transformer.py:569: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53951)\u001b[0m   return torch._transformer_encoder_layer_fwd(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for TorchTrainer_ad153ce4:\n",
      "  _time_this_iter_s: 9.941187620162964\n",
      "  _timestamp: 1681308190\n",
      "  _training_iteration: 1\n",
      "  date: 2023-04-12_10-03-11\n",
      "  done: false\n",
      "  experiment_id: 0f4b64c9d0384227be155ae00a01a646\n",
      "  hostname: Ajax\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.0.0.153\n",
      "  pid: 53910\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 13.430606365203857\n",
      "  time_this_iter_s: 13.430606365203857\n",
      "  time_total_s: 13.430606365203857\n",
      "  timestamp: 1681308191\n",
      "  timesteps_since_restore: 0\n",
      "  train_loss: 138.9628046209162\n",
      "  training_iteration: 1\n",
      "  trial_id: ad153ce4\n",
      "  val_loss: 134.1124382019043\n",
      "  warmup_time: 0.0025713443756103516\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "trainer, tune_config = training.setup_ray(train, val, BATCH_SIZE, param_space, n_tries=8)\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    trainer,\n",
    "    run_config=air.RunConfig(\n",
    "        local_dir=\"./tfmr_logger/\",\n",
    "    ),\n",
    "    tune_config=tune_config,\n",
    ")\n",
    "\n",
    "result = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e994d41d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af91bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
